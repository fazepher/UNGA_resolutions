[["index.html", "Multidimensional Ideal Points of Mexican Foreign Policy at the UNGA 1 Introduction", " Multidimensional Ideal Points of Mexican Foreign Policy at the UNGA Fernando A. Zepeda Herrera 1 Introduction knitr::opts_chunk$set(eval = FALSE) ¡OJO! DEBO CITAR PAQUETES DE R "],["data.html", "2 Data 2.1 General Assembly Session Compilation 2.2 Voting Data", " 2 Data 2.1 General Assembly Session Compilation According to the Dag Hammarskjöld Library website (Kurtas, n.d.a): For each session of the General Assembly, resolutions and decisions are compiled and issued as a supplement to the General Assembly Official Records (GAOR) […] For regular sessions, starting with the 42nd session, the document has been assigned number 49, and is Supplement 49 to the GAOR. Currently, there are usually 3 volumes As we are interested in resolutions, we would need volumes I and III which contain, respectively, those adopted by the GA in the September-December and January-September periods of each regular session. These documents are identified by their symbols A/[Sess.N.]/49Vol(I.) or A/[Sess.N.]/49Vol(III.), where [Sess.N.] is a placeholder for the session number. For example, the resolutions for the 74th session, adopted between September and December 2019 would be compiled in the document A/74/49Vol(I.) and found at https://undocs.org/en/A/74/49(Vol.I). 2.1.1 Download and Import Starting from the 67th session of the UNGA, there is a common machine readable format for these pdf. This allows us to download them (in a folder named resol_pdf) and import them separately into R each as a character vector with one element per document page. vol_1_all &lt;- list.files(here(&quot;resol_pdf&quot;)) %&gt;% keep(~str_detect(.x,&quot;Vol.I\\\\)&quot;)) %&gt;% rev() %&gt;% map(~pdf_text(here(&quot;resol_pdf&quot;,.x))) vol_3_all &lt;- list.files(here(&quot;resol_pdf&quot;)) %&gt;% keep(~str_detect(.x,&quot;Vol.III\\\\)&quot;)) %&gt;% rev() %&gt;% map(~pdf_text(here(&quot;resol_pdf&quot;,.x))) We need two different lists, one for each type of volume since the respective document structures are slightly different. For Volume I documents: The first 4 pdf pages typically contain the cover, a note on resolutions and decisions, the main table of contents, and a blank page. Numbered pages thus start at pdf page 5. An exception is document A/70/49(Vol.I) where there is no such blank page and numbered pages start at pdf page 4. The document then is split in its 7 main sections, each containing their own (sub) table of contents. Each section contains resolutions that refer to either none (first section) or one of the 6 main committees of the General Assambly (Nations 2021): First Committee (Disarmament &amp; International Security) Second Committee (Economic &amp; Financial) Third Committee (Social, Humanitarian &amp; Cultural) Fourth Committee (Special Political &amp; Decolonization) Fifth Committee (Administrative &amp; Budgetary) Sixth Committee (Legal) The document finishes with 2 annexes. For Volume III documents: The first 4 pdf pages follow the same pattern as Vol. I. documents. The document then is split in its 4 main sections. We are only interested in the first 3 as they contain resolutions: Without reference to a Main Committee In relation to the Fourth Committee (Special Political &amp; Decolonization) In relation to the Fifth Committee (Administrative &amp; Budgetary) The 4th main section contains decissions of the General Assembly. The document finishes with 2 annexes. 2.1.2 Data cleaning We shall start parsing the pdfs by extracting their main table of contents via the following function: extract_toc &lt;- function(x, vol = 1){ # We extract page 3 of the document x raw_toc &lt;- x[[3]] %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() %&gt;% str_trim() # The section for the 4th Committee is expected to be divided between lines # We then extract the table for the main sections # We assume the first two lines are &quot;Content&quot; and &quot;Section/Page&quot; # This is different depending on the volume if(vol == 1){ # In volume 1 # The 4th Committee section is divided between lines 5 and 6 of the raw toc raw_toc[5] &lt;- paste(raw_toc[5:6],collapse = &quot; &quot;) raw_toc &lt;- raw_toc[-6] # we have 7 main sections assuming (corrected) lines 3 to 9 # we also identify the page where annexes start, assuming it&#39;s line 11 clean_toc &lt;- raw_toc[3:9] %&gt;% str_split_fixed(pattern = &quot;(\\\\.{2,})&quot;, n = 2) %&gt;% as.data.frame() %&gt;% tibble() %&gt;% transmute(Section = str_trim(V1), Start_Page = str_trim(V2) %&gt;% as.integer, Annex_Page = str_extract(raw_toc[11],&quot;\\\\d+$&quot;) %&gt;% as.integer) } else { # In volume 3 # The 4th Committee section is divided between lines 4 and 5 of the raw toc raw_toc[4] &lt;- paste(raw_toc[4:5],collapse = &quot; &quot;) raw_toc &lt;- raw_toc[-5] # we have 3 main sections assuming (corrected) lines 3 to 5 # we also identify the page where decisions start, assuming it&#39;s line 6 clean_toc &lt;- raw_toc[3:5] %&gt;% str_split_fixed(pattern = &quot;(\\\\.{2,})&quot;, n = 2) %&gt;% as.data.frame() %&gt;% tibble() %&gt;% transmute(Section = str_trim(V1), Start_Page = str_trim(V2) %&gt;% as.integer, Annex_Page = str_extract(raw_toc[6],&quot;\\\\d+$&quot;) %&gt;% as.integer) } return(clean_toc) } We then do a soft check on this table to ensure that the start pages effectively refer to the correct pages in the document. In short, we verify that each given start page has as an approximate header the corresponding section and that the preceding page doesn’t. We want to identify if there is some section that doesn’t satisfy these conditions. We define the following function to help us do that: soft_check_toc &lt;- function(toc, shift = 4, all_pages = vol_1_all, n_check = 20){ toc %&gt;% mutate(Soft_Check_Start = map_chr(Start_Page, ~ all_pages[.x + shift] %&gt;% str_trim() %&gt;% str_sub(end = n_check)) %&gt;% equals(str_sub(Section, end = n_check)), Soft_Check_Prev = map_chr(Start_Page, ~ all_pages[.x + shift - 1] %&gt;% str_trim() %&gt;% str_sub(end = n_check)) %&gt;% equals(str_sub(Section, end = n_check)), Soft_Check = Soft_Check_Start &amp; !Soft_Check_Prev, i = row_number()) %&gt;% filter(!Soft_Check) } We detect that there is indeed an error on one of the Vol. I documents, that of session 75. The problem upon visual inspection is a typo in the pdf and Section II should start on page 223 not 203. We manually correct it and see that there are no further mistakes risen at this soft checkpoint. # Volume I vol_1_toc_check &lt;- seq_along(vol_1_all) %&gt;% map(~extract_toc(vol_1_all[[.x]], vol = 1) %&gt;% mutate(Session = 76 - .x, Year = 2021 - .x)) seq_along(vol_1_toc_check) %&gt;% map(~soft_check_toc(toc = vol_1_toc_check[[.x]], all_pages = vol_1_all[[.x]])) vol_1_toc_check[[1]]$Start_Page[2] &lt;- 223 soft_check_toc(toc = vol_1_toc_check[[6]], shift = 3, all_pages = vol_1_all[[6]]) seq_along(vol_1_toc_check) %&gt;% map(~soft_check_toc(toc = vol_1_toc_check[[.x]], shift = if_else(.x == 6, 3, 4), all_pages = vol_1_all[[.x]])) There are no problems identified for Vol. III documents. # Volume 3 vol_3_toc_check &lt;- seq_along(vol_3_all) %&gt;% map(~extract_toc(vol_3_all[[.x]], vol = 3) %&gt;% mutate(Session = 75 - .x, Year = 2020 - .x)) seq_along(vol_3_toc_check) %&gt;% map(~soft_check_toc(toc = vol_3_toc_check[[.x]], all_pages = vol_3_all[[.x]])) Once we are confident that we have identified correctly the starting pages of each section, we can also assign ending pages for each section and have our main table of contents object. Now, as said previously, each section contains a table of contents and the resolutions. In order to correctly parse resolution texts we will need to remove the headers and footers of the pages. These are usually the name of the section and the page number. We will thus be able to remove them programatically; there are, however, two exceptions that must be dealt with first. These are explained for Vol. I documents, but they are similar for Vol. III: The title of Section III is long enough that it’s split in two lines on its own table of contents. We must make this a single line so that our removal function works correctly. The table of contents of Section VI contains a footnote that needs to be removed. It is signaled with a star * at the end of the title and that reads Unless otherwise stated, the draft resolutions recommended in the reports were submitted by the Chair or another officer of the Bureau of the Committee. The star symbol may be registered differently depending on the specific pdf so we use regular expressions to find the correct patterns to remove. # Volume I vol_1_toc &lt;- vol_1_toc_check %&gt;% map(~.x %&gt;% mutate(End_Page = lead(Start_Page, default = unique(Annex_Page))-1) %&gt;% select(Section, Session, Year, Start_Page, End_Page)) vol_1_raw &lt;- map(vol_1_all,str_trim) # The shift is different for session 69 since there is no blank page 4 for(s in seq_along(vol_1_raw)){ print(s) aux_i &lt;- vol_1_toc[[s]]$Start_Page[3] + if_else(s == 6, 3, shift) str_detect(string = vol_1_raw[[s]][aux_i], pattern = &quot;(?&lt;=Special Political)\\n\\\\s*(?= and Decol)&quot;) %&gt;% print vol_1_raw[[s]][aux_i] &lt;- str_remove(string = vol_1_raw[[s]][aux_i], pattern = &quot;(?&lt;=Special Political)\\n\\\\s*(?= and Decol)&quot;) aux_i &lt;- vol_1_toc[[s]]$Start_Page[6] + if_else(s == 6, 3, shift) str_detect(string = vol_1_raw[[s]][aux_i], pattern = &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*(?=\\n\\\\s+\\\\d+$)&quot;) %&gt;% print str_detect(string = vol_1_raw[[s]][aux_i], pattern = &quot;(?&lt;=Committee)(\\\\*||∗)&quot;) %&gt;% print str_detect(string = vol_1_raw[[s]][aux_i], pattern = &quot;(((?&lt;=Page\\n)(\\\\*||∗)\\n)|((?&lt;=number\\n)(\\\\*||∗)\\n))&quot;) %&gt;% print vol_1_raw[[s]][aux_i] &lt;- str_remove(string = vol_1_raw[[s]][aux_i], pattern = &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*(?=\\n\\\\s+\\\\d+$)&quot;) %&gt;% str_remove(pattern = &quot;(?&lt;=Committee)(\\\\*||∗)&quot;) %&gt;% #There may be a hidden * or  at the top of the toc str_remove(pattern = &quot;(((?&lt;=Page\\n)(\\\\*||∗)\\n)|((?&lt;=number\\n)(\\\\*||∗)\\n))&quot;) } # Volume III vol_3_toc &lt;- vol_3_toc_check %&gt;% map(~.x %&gt;% mutate(End_Page = lead(Start_Page, default = unique(Annex_Page))-1) %&gt;% select(Section, Session, Year, Start_Page, End_Page)) vol_3_raw &lt;- map(vol_3_all,str_trim) for(s in seq_along(vol_3_raw)){ print(s) aux_i &lt;- vol_3_toc[[s]]$Start_Page[2] + shift str_detect(string = vol_3_raw[[s]][aux_i], pattern = paste0(&quot;(&quot;, &quot;((?&lt;=Special Political)\\n\\\\s*(?= and Decol))&quot;, &quot;|&quot;, &quot;((?&lt;=and Decolonization)\\n\\\\s*(?= Committee ))&quot;, &quot;)&quot;)) %&gt;% print vol_3_raw[[s]][aux_i] &lt;- str_remove(string = vol_3_raw[[s]][aux_i], pattern = paste0(&quot;(&quot;, &quot;((?&lt;=Special Political)\\n\\\\s*(?= and Decol))&quot;, &quot;|&quot;, &quot;((?&lt;=and Decolonization)\\n\\\\s*(?= Committee ))&quot;, &quot;)&quot;)) aux_i &lt;- vol_3_toc[[s]]$Start_Page[3] + shift str_detect(string = vol_3_raw[[s]][aux_i], pattern = paste0(&quot;(&quot;, &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*(?=\\n\\\\s+\\\\d+$)&quot;, &quot;|&quot;, &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*\\nCommittee\\\\.(?=\\n\\\\s+\\\\d+$)&quot;, &quot;)&quot;)) %&gt;% print str_detect(string = vol_3_raw[[s]][aux_i], pattern = &quot;(?&lt;=Committee)(\\\\*||∗)&quot;) %&gt;% print str_detect(string = vol_3_raw[[s]][aux_i], pattern = &quot;(((?&lt;=Page\\n)(\\\\*||∗)\\n)|((?&lt;=number\\n)(\\\\*||∗)\\n))&quot;) %&gt;% print vol_3_raw[[s]][aux_i] &lt;- str_remove(string = vol_3_raw[[s]][aux_i], pattern = paste0(&quot;(&quot;, &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*(?=\\n\\\\s+\\\\d+$)&quot;, &quot;|&quot;, &quot;_+\\n(\\\\*||∗)\\n*[[:print:]]*\\nCommittee\\\\.(?=\\n\\\\s+\\\\d+$)&quot;, &quot;)&quot;)) %&gt;% str_remove(pattern = &quot;(?&lt;=Committee)(\\\\*||∗)&quot;) %&gt;% #There may be a hidden * or  at the top of the toc str_remove(pattern = &quot;(((?&lt;=Page\\n)(\\\\*||∗)\\n)|((?&lt;=number\\n)(\\\\*||∗)\\n))&quot;) } Once we have cleaned this exceptions we would programmatically remove headers and footers via the following function: remove_header_footer &lt;- function(raw_page, header){ # Ensure we have a trimmed page raw_page &lt;- str_trim(raw_page) # Verify headers, if they don&#39;t match, return NA end_header &lt;- nchar(header) check_header &lt;- str_sub(string = raw_page, end = end_header) == header end_header &lt;- if_else(check_header, end_header, NA_integer_) # The footer is usually the document page on its own line, # The function returns NA if it doesn&#39;t locate this pattern. start_footer &lt;- str_locate(raw_page,&quot;\\n\\\\s*\\\\d+$&quot;)[,&quot;start&quot;] new_page &lt;- str_sub(raw_page,start = end_header + 1, end = start_footer - 1) %&gt;% unlist() %&gt;% str_trim() return(new_page) } We can construct a master_tibble object that integrates, for each section and still in raw format, its own table of contents as well as all the resolutions. vol_1_master_tibble &lt;- seq_along(vol_1_toc) %&gt;% map(function(s) vol_1_toc[[s]] %&gt;% mutate(Raw_Pages = pmap(list(Start_Page, End_Page, Section), ~ vol_1_raw[[s]][..1:..2 + if_else(s == 6, 3, shift)] %&gt;% discard(~.x == &quot;&quot;) %&gt;% remove_header_footer(paste0(..3,&quot;\\n&quot;))), First_Resol = map2_int(Raw_Pages,Session, ~str_detect(.x,paste(&quot;^RESOLUTION&quot;,.y)) %&gt;% which %&gt;% min), Section_toc = map2_chr(Raw_Pages,First_Resol, ~.x[1:(.y-1)] %&gt;% paste(collapse = &quot;\\n&quot;)), Section_res = map2_chr(Raw_Pages,First_Resol, ~.x[.y:length(.x)] %&gt;% paste(collapse = &quot;\\n&quot;))) %&gt;% select(-Raw_Pages,-First_Resol)) vol_3_master_tibble &lt;- seq_along(vol_3_toc) %&gt;% map(function(s) vol_3_toc[[s]] %&gt;% mutate(Raw_Pages = pmap(list(Start_Page, End_Page, Section), ~ vol_3_raw[[s]][..1:..2 + shift] %&gt;% discard(~.x == &quot;&quot;) %&gt;% remove_header_footer(paste0(..3,&quot;\\n&quot;))), First_Resol = map2_int(Raw_Pages,Session, ~str_detect(.x,paste(&quot;^RESOLUTION&quot;,.y)) %&gt;% which %&gt;% min), Section_toc = map2_chr(Raw_Pages,First_Resol, ~.x[1:(.y-1)] %&gt;% paste(collapse = &quot;\\n&quot;)), Section_res = map2_chr(Raw_Pages,First_Resol, ~.x[.y:length(.x)] %&gt;% paste(collapse = &quot;\\n&quot;))) %&gt;% select(-Raw_Pages,-First_Resol)) 2.1.2.1 Resolution Texts Once we separated the table of contents from the resolution texts we can parse them. A key observation is that each resolution is signaled by an upper letter heading such as RESOLUTION 75/1, RESOLUTIONS 75/101 A and B or RESOLUTIONS 75/254 A-C depending on whether it’s a standalone resolution or it has two or more subresolutions. Then, each resolution contains metadata on it’s adoption such as session, date, voting data, or sponsorship. The actual resolutions start by it’s resolution number and a period on its own line (e.g. 75/1.). We use these patterns to separate all resolutions and identify the text as well as the metadata. vol_1_resol_text_df &lt;- vol_1_master_tibble %&gt;% map_dfr(~ .x$Section_res %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% str_split(pattern = &quot;(?=RESOLUTION)&quot;) %&gt;% unlist %&gt;% tibble(Resolution_Text = .) %&gt;% slice(-1) %&gt;% separate(col = Resolution_Text, into = c(&quot;Resolution_Header&quot;,&quot;Aux&quot;), sep=&quot;\\n&quot;, extra = &quot;merge&quot;) %&gt;% mutate(UNRES = str_extract(Resolution_Header,&quot;\\\\d+/\\\\d+&quot;), Aux = str_trim(Aux), Aux_Pos = map2(Aux,UNRES,~str_locate(.x, pattern = paste0(.y,&quot;\\\\.\\\\s+&quot;))), Start_Pos = map_int(Aux_Pos,~.x[1,&quot;start&quot;]), End_Pos = map_int(Aux_Pos,~.x[1,&quot;end&quot;]), Resol_Meta = map2_chr(Aux,Start_Pos,~str_sub(.x,end = .y - 1)), Resol_Text = map2_chr(Aux,End_Pos, ~ str_sub(.x,start = .y + 1)), Resol_Text_WPOP = Resol_Text %&gt;% str_remove_all(&quot;(?&lt;=\\n)\\\\s+[A-Z][a-z]*ing&quot;) %&gt;% str_remove_all(&quot;(?&lt;=\\n)\\\\s+\\\\d+\\\\.\\\\s+[A-Z][a-z]*&quot;)) %&gt;% select(UNRES,Resol_Meta,Resol_Text,Resol_Text_WPOP)) # Double check hidden characters and RES/73/293 vol_3_resol_text_df &lt;- vol_3_master_tibble %&gt;% map_dfr(~ .x$Section_res %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% str_split(pattern = &quot;(?=RESOLUTION)&quot;) %&gt;% unlist %&gt;% tibble(Resolution_Text = .) %&gt;% slice(-1) %&gt;% separate(col = Resolution_Text, into = c(&quot;Resolution_Header&quot;,&quot;Aux&quot;), sep=&quot;\\n&quot;, extra = &quot;merge&quot;) %&gt;% mutate(UNRES = str_extract(Resolution_Header,&quot;\\\\d+/\\\\d+&quot;), Aux = str_trim(Aux), Aux_Pos = map2(Aux,UNRES,~str_locate(.x, pattern = paste0(.y,&quot;\\\\.\\\\s+&quot;))), Start_Pos = map_int(Aux_Pos,~.x[1,&quot;start&quot;]), End_Pos = map_int(Aux_Pos,~.x[1,&quot;end&quot;]), Resol_Meta = map2_chr(Aux,Start_Pos,~str_sub(.x,end = .y - 1)), Resol_Text = map2_chr(Aux,End_Pos, ~ str_sub(.x,start = .y + 1)), Resol_Text_WPOP = Resol_Text %&gt;% str_remove_all(&quot;(?&lt;=\\n)\\\\s+[A-Z][a-z]*ing&quot;) %&gt;% str_remove_all(&quot;(?&lt;=\\n)\\\\s+\\\\d+\\\\.\\\\s+[A-Z][a-z]*&quot;)) %&gt;% select(UNRES,Resol_Meta,Resol_Text,Resol_Text_WPOP)) There is, however, a last cleaning in the column Resol_Text_WPOP. This name referes to resolution texts without preambulatory and operative phrases. These are common phrases that are always present in UNGA resolutions since they indicate the structure explained here (Kurtas, n.d.b). As the next step in the analysis will be text mining, we could consider all these phrases as noise within the UNGA context so it is useful to have two versions of the resolutions, one without them and one original that keeps them. We have now completed the main process of data cleaning, we can join all the resolutions and save the data. resol_text_df &lt;- bind_rows(mutate(vol_1_resol_text_df, Vol = &quot;1&quot;), mutate(vol_3_resol_text_df, Vol = &quot;3&quot;)) %&gt;% mutate(ID_FAZH = paste(Vol,UNRES,sep=&quot;/&quot;)) save(resol_text_df, file = here(&quot;resol_data&quot;,&quot;resol_text_df.RData&quot;)) 2.2 Voting Data FALTA EXPLICAR DATOS DE VOTOS While we could parse voting data from the metadata of resolutions in the compilation pdfs. We can recover an already processed data set on voting records for the UNGA (Voeten, Strezhnev, and Bailey 2021): Accesed and downloaded on June 15, 2021 available at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/LEJUQZ Erik Voeten, “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013) Bailey, Michael A., Anton Strezhnev, and Erik Voeten. 2017. Estimating dynamic state preferences from united nations voting data. Journal of Conflict Resolution 61 (2): 430-56. "],["topic-modelling.html", "3 Topic Modelling 3.1 LDA 3.2 CTM 3.3 Pipeline 3.4 Parameters considered 3.5 Exploration of Topics", " 3 Topic Modelling 3.1 LDA EXPLAIN LDA THEORY HERE 3.2 CTM EXPLAIN CTM THEORY HERE 3.3 Pipeline In order to come up with topics for the resolutions at the UNGA, we need to explicitly define a text mining pipeline. That is, there are several steps and decisions to be made that allows one to come up with a given matrix \\(\\Lambda\\) of topic proportions for resolutions. Data: We could restrict ourselves to a given subset of resolutions, use the complete 2012-2020 data we have thus far, or extend the analysis once we have more data available. Document Texts: Once we have settled on the period or resolutions considered, we have to decide if we wish to use the raw version of the text or if we are happy with the simplified version that removed preambulatory and operational phrases (POP). While this step may be considered a special version of removing stopwords, it comes first in the pipeline since POP may be considered part of the structure of a UN resolution more than stopwords. Corpus: With a given data set and document texts we can construct a corpus. Tokenization: An important part of text mining is to separate documents into tokens which can be thought of as its basic elements. These are usually words, but may include punctuation, symbols or spaces. That is why, when converting our resolutions into tokens we have to decide if we remove punctuation, symbols, numbers, and spaces. Stopwords: There are some words in a given language that are considered noise and judged not to convey any substantial meaning to a text mining analysis. Examples usually include articles or pronouns. These are called stopwords and may be removed prior to any modelling. One should decide which are the stopwords and whether or not to remove them. Given a particular context one may wish to either modify existing general lists of a given language stopwords or extend them by identifying domain specific stopwords. Collocations: Another important step is to decide whether or not to consider collocations which are pairs, triplets or, in general, \\(n\\)-length consecutive words that convey a distinct meaning when considered as a single word. Examples are names, such as United Nations or International Court of Justice. To decide whether or not an expression constitutes a collocation one has to decide the lengths or sizes to consider, i.e. whether we are looking for two-words expressions, or \\(n\\)-words expressions in general. Also, one has to set a minimum number of occurences of the expression to be considered as a collocation. Once those expressions satisfying both conditions are identified a criteria must be set to decide if they are indeed a collocation that conveys a distinct meaning. The used criteria is a \\(z\\) score bigger than 3 as costumary in quanteda analyses (textstat_collocations has more details). Final Tokens: Before considering models one may wish to also exclude some other tokens. For example, one may wish to consider only tokens with given lengths, removing or not one or two characters tokens. Additionally, one may establish a minimum token frequency. Topic Model: Only after all the previous steps have been completed we can fit a model to the final dataset. The models considered here can be defined by the number of topics \\(k\\) and its type, LDA or CTM, as well as possibly some control parameters for the computer algorithm used to fit them. These last parameters include, for example, the seed or the maximum number of iterations. This process is implemented in the following pipeline function: fit_topic_model &lt;- function(model, k, data = resol_text_df, text_field = &quot;Resol_Text_WPOP&quot;, docid_field = &quot;ID_FAZH&quot;, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, en_stopwords = stopwords(&quot;en&quot;,source = &quot;marimo&quot;), coll_size = 2:5, coll_min_count = 15, unga_stopwords = unga_stopwords, remove_1_2l_words = TRUE, min_term_freq = 15, seed = 51295, ...){ corpus_unga &lt;- data %&gt;% corpus(text_field = text_field, docid_field = docid_field) tokenized_unga &lt;- corpus_unga %&gt;% tokens(remove_punct = remove_punct, remove_symbols = remove_symbols, remove_numbers = remove_numbers, padding = T) %&gt;% tokens_remove(pattern = en_stopwords, padding = TRUE) if(remove_1_2l_words){ tokenized_unga &lt;- tokenized_unga %&gt;% tokens_keep(min_nchar = 3) } collocations_nested &lt;- tokenized_unga %&gt;% textstat_collocations(size = coll_size, min_count = coll_min_count) tokenized_unga_col &lt;- tokenized_unga %&gt;% tokens_compound(pattern = collocations_nested[collocations_nested$z &gt; 3]) tm_unga &lt;- tokenized_unga_col %&gt;% tokens_remove(unga_stopwords) %&gt;% dfm() %&gt;% dfm_trim(min_termfreq = min_term_freq) %&gt;% convert(to = &quot;topicmodels&quot;) set.seed(seed) if(model == &quot;LDA&quot;){ print(&quot;Model: LDA&quot;) model_unga &lt;- mclapply(k, LDA, x = tm_unga, ...) } else{ print(&quot;Model: CTM&quot;) model_unga &lt;- mclapply(k, CTM, x = tm_unga, ...) } return(list(fitted_model = model_unga, call = sys.call())) } 3.4 Parameters considered With the pipeline function we can now consider several models by varying its parameters. First, we can mention some parameters that will remain the same for all the models considered. These are: data: Complete 2012-2020 dataset text_field: We are removing POP words Basic removal parameters: We remove punctuation, symbols, and numbers. Stopwords: we are removing three types of stopwords The marimo English stopwords after collocations and a trial frequency analysis, the following context specific words The context specific stopwords “including,”“resolution,”“implementation,” “united_nations,”“secretary-general,”“general_assembly.” These were identified in trial analyses. Spaces and the footnotes separator symbol “_______________” coll_size: Valid lengths of collocations are 2 to 5 Control parameters: all algorithms used the same seed, maximum iteration parameters, tolerance, and cores. Then, we varied the following parameters Minimum frequencies: 3 pairs of minimum length for collocations and term frequencies for analyses were considered (coll_min_count, min_term_freq): (10, 20), (50, 50), and (100, 75). Removal of 1 and 2 letter words or not. Model: type LDA or CTM. Number of topics \\(k\\): from 2 through 16. This meant in total 180 models (3 x 2 x 2 x 15) 3.5 Exploration of Topics EXPLAIN EXPLORATION OF TOPICS ALONG WITH GOOD PLOTS "],["item-response-theory.html", "4 Item Response Theory 4.1 Basic concepts and models 4.2 STILL QUESTIONS ON", " 4 Item Response Theory 4.1 Basic concepts and models The fundamental idea underpinning Item Response Theory (IRT) is that one attempts to “measure something that [does] not exist as a directly observable or manifest variable” (Linden 2016a) but is instead latent. In many applications, mostly from educational and psychometric domains, this latent variable is considered an ability and one intends to measure it via several items or questions related to it (Linden 2016b ; Baker and Kim 2017). The way items help measure abilities is that it is generally thought that people with higher abilities are more likely to answer correctly items intended to measure said ability. More precisely, suppose we define a given item’s difficulty as \\(b\\) on the ability scale and a given person’s ability as \\(\\theta\\) on the same scale. We would say that people with higher abilities than the item’s difficulty, \\(\\theta &gt; b\\), are more likely to answer it correctly. On the contrary, a person whose ability level is smaller than the item’s difficulty , \\(\\theta &lt; b\\), will have a low probability of answering correctly. Moreover, what we would actually like to say is that there exists an increasing probability of answering correctly with the difference between an individual’s ability and an item’s difficulty. We could express this notion for example as \\[P(Y = 1 | \\theta, b) = F(\\theta - b)\\] where \\(Y=1\\) represents a correct answer and \\(F\\) is an increasing cumulative distribution function (San Martín 2016). Several functions are available, amongst the most popular are the logistic, normal, and extreme value of the Gumbell type (Albert 2016). 4.1.1 1PL or Rasch Model Now, we are usually interested in measuring several individuals and have more than one item to do so. Let us say we have \\(J\\) individuals with abilities \\(\\theta_j\\) for \\(j = 1, 2, \\dots, J\\) and \\(I\\) items with difficulties \\(b_i\\) for \\(i=1, 2, \\dots, I\\). We would also have \\(N\\) answers \\(Y_k\\), \\(k=1,2,\\dots,K\\) which are either correct or incorrect for a given individual-item pair. If we have a logistic response function our basic IRT model could look like this: \\[Y_k|\\theta, b \\overset{ind}{\\sim} Ber(p_k) \\qquad p_k = P(Y_k = 1 | \\theta, b) = \\dfrac{\\exp(\\theta_{j[k]} - b_{i[k]})}{1 - \\exp(\\theta_{j[k]} - b_{i[k]})}\\] for \\(k = 1, 2, \\dots, K\\) and where the notation \\(j[k]\\) and \\(i[k]\\) identify the correct individual and item indexes of the \\(k\\)-th answer. It is usually called the 1PL (1 item Parameter Logistic) or Rasch model. 4.1.2 2PL and Normal Ogive Model There are other models that introduce an additional item parameter beyond its difficulty. These are normally denoted by \\(a_i\\) and called the discrimination parameters since they signal how much an item discriminates between individuals with abilities slightly higher or lower than the difficulty of the item. This so-called 2PL model is a generalization of the 1PL: \\[p_k = P(Y_k = 1 | \\theta, a, b) = \\dfrac{\\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}{1 - \\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}\\] for \\(k = 1, 2, \\dots, K\\). Some comments are in order: 1PL is a special case where all \\(a_i = 1\\). Graphically, we can see that bigger values of \\(a_i\\) indicate a sharper curve that goes quicker to 1 or 0, whereas lower magnitudes yield flatter curves. A negative value of \\(a_i\\) would indicate that higher able individuals are less likely to answer correctly. This is generally indicative of a bad test item in educational or psychometric tests but in other contexts it may be reasonable. One can think of \\(a_i\\) as scaling, that is magnifying or reducing, differences between abilities and difficulties. Another way of representing the model is with a linear predictor \\(\\eta_k = \\alpha_{i[k]} + \\beta_{i[k]}\\theta_{j[k]}\\), this is a notation that resembles traditional logistic regression and where \\(\\alpha_{i[k]} = a_{i[k]}b_{i[k]}\\) and \\(\\beta_{i[k]} = a_{i[k]}\\). There is no absolute restriction in that the response or link function has to be logistic. In fact, another famous model, proposed by Lord, is the Normal Ogive model which has the same structure except that it replaces the logit link for the probit, that is the logistic cdf for the normal cdf, hence the name. 4.1.3 3PL or 4PL A third additional parameter may be a pseudo-guessing parameter in which respondents are allowed to have a correct answer by virtue of “guessing” regardless of their abilities. This is a common model in the IRT literature and is expressed as: \\[p_k = P(Y_k = 1 | \\theta, a, b, c) = c_{i[k]} + (1-c_{i[k]})\\dfrac{\\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}{1 - \\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}\\] for \\(k = 1, 2, \\dots, K\\). (Bafumi et al. 2005/ed) propose what they call a robust logistic regression model and that we could call the 4PL in which in addition to the pseudoguessing parameter, they also allow random mistakes regardless of abilities. This is a robust model in the sense that they look to allow some outliers in which either low ability respondents answer correctly to items they were not supposed to or high ability respondents exhibit some mistakes that their ability level wouldn’t consider likely. Their expression would be akin to \\[p_k = P(Y_k = 1 | \\theta, a, b, c, d) = c_{i[k]} + (1-c_{i[k]} - d_{i[k]})\\dfrac{\\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}{1 - \\exp\\left\\lbrace a_{i[k]}(\\theta_{j[k]} - b_{i[k]})\\right\\rbrace}\\] for \\(k = 1, 2, \\dots, K\\). 4.1.4 MIRT One could extend IRT models to consider several dimensions of the latent abilities (Swaminathan and Rogers 2016; Mark D. Reckase 2016; M. D. Reckase 2009). For example, a generalization of the Normal Ogive model in its linear predictor form would be similar in appearence to a probit regression: \\[p_k = P(Y_k = 1 | \\theta, \\alpha, \\beta) = \\Phi\\left[ \\alpha_{i[k]} + \\beta_{i[k],1}\\theta_{j[k],1} + \\beta_{i[k],2}\\theta_{j[k],2} + \\dots + \\beta_{i[k],D}\\theta_{j[k],D}\\right]\\] where now, instead of a scalar ability \\(\\theta_{i}\\) each individual is positionned on a \\(D\\)-dimensional ability coordinate \\(\\theta_{i} = (\\theta_{i, 1}, \\theta_{i, 2}, \\dots, \\theta_{i, D})\\). Each item would have associated an intercept \\(\\alpha_j\\) parameter and a corresponding \\(D\\)-dimensional coefficients vector \\(\\beta_{j} = (\\beta_{j, 1}, \\beta_{j, 2}, \\dots, \\beta_{j, D})\\). Reckase calls this, and its logistic version, a compensatory MIRT model because an examinee whose location in the space has a high value for one coordinate can balance out a low value for another coordinate to yield the same probability of the item score as an examinee who is located aat a place represented by coordinates of roughly equal magnitude. (Mark D. Reckase 2016) 4.2 STILL QUESTIONS ON 4.2.1 Latent variable form QUESTION: IS IT CORRECT TO SAY THAT THIS FORMULATION WOULD BE EQUIVALENT TO HAVING A Z LATENT VARIABLE PLUS ERROR AND THE CHOICE OF ERROR DISTRIBUTION DETERMINES THE LINK: \\[Y = 1 \\Longleftrightarrow Z &gt; 0 \\qquad Z = \\eta + \\epsilon\\] Where \\(\\eta\\) may be of the Rasch form (\\(\\eta = \\theta - b\\)), the Lord form (\\(\\eta = a(\\theta - b)\\)) or the MIRT form (\\(\\eta = \\alpha + \\underline{\\beta}&#39;\\underline{\\theta}\\)), and \\(\\epsilon\\) is interpreted as normal, logistic or Gumbell (assymetric) error. 4.2.2 Identifiability QUESTIONS: I AM AWARE OF THE FOLLOWING IDENTIFICATION ISSUES IN THE 2PL OR NORMAL OGIVE MODEL, mostly from reading (Bafumi et al. 2005/ed; San Martín 2016; Bailey, Strezhnev, and Voeten 2017; Lauderdale and Clark 2014) ADDITIVE NON-IDENTIFIABILITY/LOCATION: translating the scale, adding and subtracting same constant to abilities, difficulties MULTIPLICATIVE/ SCALE: multiplying and dividing discriminations and abilitiy differences REFLECTION: Changing all the signs of discrimination parameters It seems, from (San Martín 2016) that a necessary and sufficient condition for identification on 2PL would be to set arbitrarly one difficulty and one discrimination to \\(1\\). (Bafumi et al. 2005/ed) recommend some approaches based on priors, they seem to include a unanimous vote/item with all correct answers to help with this (p. 181), or using some adjusted parameters by normalizing all of them via the mean and standard deviation of estimated abilities. But I’m not sure how this translates to the MIRT, for example, (Lauderdale and Clark 2014) seem to talk about a ROTATION non-identifiability akin to the REFLECTION one and say they explicitly don’t allow, for example, people voting in favor of a right-wing opinion economically but left-wing opinion socially. I don’t know exactly how they accomplish this on their models especially considering they end up having way more than 2 dimensions. "],["proposed-model.html", "5 Proposed Model 5.1 IRT Terminolog equivalence 5.2 LC model 5.3 Proposed extension", " 5 Proposed Model In the application for foreign policy ideal points we would like to extend the model of (Lauderdale and Clark 2014) with an application to UNGA resolutions. 5.1 IRT Terminolog equivalence In the terminology of IRT we have abilities in dimensions for respondents, items as questions, and answers as pairs of a given respondent and item. Lauderdale and Clark (LC) have justices (respondents), opinions of the court (items), and votes (answer/pair). We would have countries, resolutions, and votes. While our votes are polytomous (yea, abstain, nay), we will dichotomize first (vote yes, vote abstain or nay). The dimensions in IRT are associated with topics extracted from the topic modelling of the items: that is opinions for LC and resolutions in our case. LC introduce the Topic Modeling parameter matrix \\(\\Lambda\\) that we could call the dimension or topic mixture proportions. We have, for each item/opinion/resolution a \\(\\lambda_i = (\\lambda_{i,1},\\lambda_{i,2}, \\dots, \\lambda_{i,D})\\) row vector of proportions that correspond to each of the \\(D\\) topics or dimensions. They sum to 1 for each row: \\(1 = \\sum_d \\lambda_{i,d}\\) for all \\(i\\). These are the parameters coming from LDA or CTM and are point estimators that will be considered as fixed data from here on. 5.2 LC model I think there are two (initial) ways of seeing the LC model. One is as an extension of the Normal Ogive MIRT in which they decouple the item parameters. Another one is to say they have a Normal Ogive model in which each answer/vote has its own single dimension which is constructed from a given mixture of topic dimensions and justices positions. So, this is the Normal Ogive MIRT \\[p_k = P(Y_k = 1 | \\theta, \\alpha, \\beta) = \\Phi\\left[ \\alpha_{i[k]} + \\beta_{i[k],1}\\theta_{j[k],1} + \\beta_{i[k],2}\\theta_{j[k],2} + \\dots + \\beta_{i[k],D}\\theta_{j[k],D}\\right]\\] LC instead decouples the item parameters as \\[\\beta_{i,d} = \\beta_{i}\\lambda_{i,d}\\] so that the new model is \\[p_k = P(Y_k = 1 | \\theta, \\alpha, \\beta, \\Lambda) = \\Phi\\left[ \\alpha_{i[k]} +\\sum\\limits_{d = 1}^D \\beta_{i[k]}\\;\\lambda_{i[k],d}\\;\\theta_{j[k],d}\\right]\\] The second way of viewing the LC model is to think of each answer as having its own “mixed” dimension with a typical Normal Ogive unidimensional form: \\[p_k = P(Y_k = 1 | \\theta, \\alpha, \\beta, \\Lambda) = \\Phi\\left[ \\alpha_{i[k]} +\\beta_{i[k]}\\tilde{\\theta}_{k}\\right]\\] where each answer-specific dimension is a “mixture” of the person positions on the \\(D\\) topic dimensions according to the topic proportions \\(\\lambda\\). \\[\\tilde{\\theta}_{k} = \\sum\\limits_{d = 1}^D \\lambda_{i[k],d}\\;\\theta_{j[k],d}\\] 5.3 Proposed extension Theirs is a static model, but in order to explore the evolution of Mexican foreign policy positions we need a dynamic model. So far, my first idea for what seems to me as a natural dynamic extension is the following: \\[Y_k|\\theta, b \\overset{ind}{\\sim} Ber(p_k)\\] for all \\(k = 1, 2, \\dots, K\\), where \\[p_k = P(Y_k = 1 | \\Theta, \\alpha, \\beta, \\Lambda) = \\Phi\\left[ \\alpha_{i[k]} +\\beta_{i[k]}\\tilde{\\theta}_{k}\\right].\\] Here, the \\(\\alpha_{i[k]}\\) and \\(\\beta_{i[k]}\\) define one intercept and one coefficient for each of the \\(I\\) resolutions. Each country then has their own pseudo-position, \\(\\tilde{\\theta}_{k}\\), for a given resolution and it is defined as \\[\\tilde{\\theta}_{k} = \\sum\\limits_{d = 1}^D \\lambda_{i[k],d}\\;\\theta_{j[k],t[k],d}.\\] Where \\(\\lambda_{i[k],d}\\) are the topic mixture proportions of the resolution and \\(\\theta_{j[k],t[k],d}\\) the country’s position on the topic dimensions at the time of the resolution. These positions evolve in time via \\[\\theta_{j,t,d} = \\theta_{j, t - 1, d} + \\epsilon_{j,d}\\] from a given starting point \\(\\theta_{j, 0, d}\\) and country-dimensional evolution shocks \\(\\epsilon_{j,d}\\). Some reflections that I still need to address: What are the general identifiability problems. I think that the \\(\\lambda\\) data help with the identification issues but I’m not sure how much. My logic is that the specific values of a resolution help with the relative positioning of resolutions between them. We could arbitrarily set Mexico’s positions at the origin of the \\(D\\)-dimensional space at time \\(0\\) and set the intercept of one resolution also as \\(0\\), but I’m not sure if that is enough. I know that the eventual prior distributions could help with this issue, but I think it’s best to address it in a more principled way. Both (Bafumi et al. 2005/ed) and (Lauderdale and Clark 2014) cite a Douglas Rivers 2003 working paper entitled “Identification of Multidimensional Spatial Voting Models” that I don’t seem able to find because it is unpublished. What would other good references on this topic are there. For example, (San Martín 2016) only talks about one-dimensional IRT models but it does go to great lengths at explaining several issues and proposing identifiability constraints. Should evolution shocks be pooled accross countries and dimensions? That is, for example, \\(\\epsilon_{j,d} \\overset{iid}{\\sim} N(0,\\sigma_\\epsilon^2)\\) for all \\(j, d\\)? A priori I would think that different countries and may have different evolution dynamics so that maybe each country’s \\(d\\) dimension evolutions should come from the same distribution but different countries should have different distributions. This may have an effect also on identification. We could also have a multivariate normal representation of evolutions. At first one could have independent evolutions but perhaps it is possible to allow for correlated evolutions for a given country across dimensions under the logic that if one country changes positions in one dimension it would be likely that it also changed its positions on other dimensions. Once again, I have some doubts as to what this would mean in terms of identification constraints. "],["references.html", "6 References", " 6 References "]]
